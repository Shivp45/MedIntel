#  Medical Research Assistant for Doctors

##  Project Description
An AI-powered medical assistant designed for doctors to ask clinical questions and receive **grounded, evidence-based answers** using **Retrieval-Augmented Generation (RAG)** and hybrid **LLM fallback orchestration**.

Medical PDFs placed in the `data/` directory are automatically extracted, chunked, embedded, indexed in **ChromaDB**, and retrieved via **semantic similarity search** to minimize hallucinations. Answers are generated using **Llama 3.1-8B via Groq API** with **OpenAI GPT-4o-mini fallback** for robustness.

---

## ğŸ§  How it Works

**Ingestion:**  
Medical documents in `data/` are auto-loaded, text-extracted, chunked (1500 characters with 300 overlap), embedded using a Sentence Transformer model, and stored in a persistent **ChromaDB vector index**.

**Retrieval:**  
Doctor queries are converted into embeddings and matched against the **vector index** using semantic similarity search. The system retrieves **Top 5 most relevant medical literature chunks** with metadata and relevance scores.

**Hybrid Generation:**  
- **Primary:** Generates answers using **Llama 3.1-8B via Groq API**, strictly grounded in retrieved medical context  
- **Fallback:** Automatically switches to **OpenAI (GPT-4o-mini)** if Groq is unavailable or fails  
- If retrieved documents lack sufficient information, the model responds with a **clear medical disclaimer instead of hallucinating**

---

## ğŸ§© Tech Stack

- **LangChain:** RAG orchestration framework  
- **ChromaDB:** Vector storage and semantic retrieval  
- **Hugging Face Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`  
- **LLM APIs:** Groq (Llama 3.1-8B-Instant) + OpenAI (GPT-4o-mini fallback)  
- **PDF Processing:** PyMuPDF (`fitz`)  

---

## âœ¨ Key Features

 **Clinical Context Priority:** Answers only from retrieved medical PDFs  
 **Hybrid RAG:** Falls back to OpenAI if Groq fails  
 **Source Attribution:** Retrieved medical sources displayed in UI + logs  
 **Optimized Retrieval:** 1500-char chunks for full clinical meaning  
 **Drug Safety Alerts:** Highlights drug interactions, adverse effects, contraindications  
 **Robust LLM Fallback:** Auto-switches to OpenAI if Groq fails  
 **Utilities:** python-dotenv, tqdm, numpy, pandas (optional for eval scripts).

---



## ğŸ§  Core Modules Included
| Module | Purpose |
|---|---|
| `ingest_data.py` | Extracts text from PDFs â†’ splits â†’ embeds â†’ stores in ChromaDB |
| `verify_index.py` | Validates vector index integrity and tests similarity search |
| `rag_pipeline.py` | Embeds doctor query â†’ retrieves top 5 chunks â†’ calls LLM APIs with fallback |
| `evaluate_rag.py` | Benchmarks retrieval quality + answer grounding using RAG metrics |
| `app.py` | Streamlit UI for doctor interaction + source preview + safety alerts |

---



# Images 

Image1
![Image2](./public/Image1.png)  

Image2
![Image2](./public/Image2.png)  

Image3
![Image3](./public/Image3.png)  

Image4
![Image4](./public/Image4.png)  

Image5
![Image5](./public/Image5.png)  





# Installation and Setup

1ï¸âƒ£ Clone the Repository 

```
    git clone https://github.com/Shivp45/MedIntel.git  
    cd MedIntel
```

2ï¸âƒ£ Activate your virtual environment

```
    python -m venv ai_service  
    ai_service\Scripts\activate
```

3ï¸âƒ£ Install dependencies

```
    pip install -r requirements.txt
```

4ï¸âƒ£ Add medical PDFs  
Copy your medical research papers, guidelines, toxicology, drug docs into:

```
    data/*/*
```
5ï¸âƒ£ Run document ingestion (creates vector DB + index)  
Only if you add new data file/files...

```
    python backend/ingest_data.py   
```
Expected output should show:  
```
    âœ” 32 PDFs detected  
    âœ” ~4688 chunks embedded  
    âœ” Vector DB stored in embeddings/chroma/ (with chroma.sqlite3 inside it)
```

6ï¸âƒ£ (Optional) Verify vector index is working  

```
    python verify_index.py
```

This will test semantic search like:  

```
    "What are common medical treatments?" â†’ returns 3 similar chunks
```

7ï¸âƒ£ Run the doctor chat UI  

```
    streamlit run app.py
```

The app will open automatically in your browser at:  

```
    http://localhost:8501
```    



# Flow Diagram  

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  data/ (PDF files)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ingest_data.py      â”‚
                    â”‚  - Extract text      â”‚
                    â”‚  - Chunk docs        â”‚
                    â”‚  - Generate embeds   â”‚
                    â”‚  - Store in Chroma   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ embeddings/chroma/ (DB)   â”‚
                    â”‚  Vector Index Stored Here  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  rag_pipeline.py     â”‚
                    â”‚  - Embed query       â”‚
                    â”‚  - Retrieve top K    â”‚
                    â”‚  - Send context â†’ LLMâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Groq API (Primary LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                Model: Llama 3.1-8B                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚  (if fails 401/timeout)â”‚
                         â–¼                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OpenAI API (Fallback LLM) â”‚
         â”‚           Model: GPT-4o-mini          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  app.py (Streamlit UI)â”‚
                    â”‚  - Show answer        â”‚
                    â”‚  - Show sources       â”‚
                    â”‚  - Safety alerts      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ evaluate_rag.py      â”‚
                    â”‚ - Retrieval metrics  â”‚
                    â”‚ - Keyword grounding  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ verify_index.py      â”‚
                    â”‚ - Test vector search â”‚
                    â”‚ - Check DB integrity â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



[Doctor Query] â†’ [Embed Query]
       â†“
[Retrieve Top 5 Chunks from ChromaDB]
       â†“
[Send Context + Query to Groq (Llama 3.1-8B)]
       â†“
{If fails}
       â†“
[OpenAI Fallback (GPT-4o-mini)]
       â†“
[Return Answer + Sources + Safety Warnings]
       â†“
[Display in Streamlit UI]